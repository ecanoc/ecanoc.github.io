[{"authors":["admin"],"categories":null,"content":"I am a senior scientist at AudioSourceRe in Ireland, where I develop music source separation technologies. My PhD work started off from my interest to apply MIR technologies in music education. Since then, I have worked in sound source separation, music information retrieval, computational musicology, audio processing, among other research topics. Since 2012, I am the CSO and co-founder of Songquito, a small company that builds music education apps such as [Song2See] (https://www.song2see.com). I previously worked at A*STAR in Singapore, and at Fraunhofer IDMT in Germany. I have worked in a number of applied research projects with industry partners, as well as in European, German, and Asian research projects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ecanoc.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a senior scientist at AudioSourceRe in Ireland, where I develop music source separation technologies. My PhD work started off from my interest to apply MIR technologies in music education. Since then, I have worked in sound source separation, music information retrieval, computational musicology, audio processing, among other research topics. Since 2012, I am the CSO and co-founder of Songquito, a small company that builds music education apps such as [Song2See] (https://www.","tags":null,"title":"","type":"authors"},{"authors":["Juan Sebastián Gómez Cañón","Estefanía Cano","Perfecto Herrera","Emilia Gómez"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"5f466bcd2a3e946ad40a6f901976f376","permalink":"https://ecanoc.github.io/publication/eusipco20-2/","publishdate":"2020-06-01T19:28:09+08:00","relpermalink":"/publication/eusipco20-2/","section":"publication","summary":"In this study, we address emotion recognition using unsupervised feature learning from speech data, and test its transferability to music. Our approach is to pre-train models using speech in English and Mandarin, and then fine-tune them with excerpts of music labeled with categories of emotion. Our initial hypothesis is that features automatically learned from speech should be transferable to music. Namely, we expect  the intra-linguistic setting (e.g., pre-training on speech in English and fine-tuning on music in English) should result in improved performance over the cross-linguistic setting (e.g., pre-training on speech in English and fine-tuning on music in Mandarin). Our results confirm previous research on cross-domain transferability, and encourage research towards language-sensitive Music Emotion Recognition (MER) models.","tags":["Deep learning","Music Emotion Recogntion","Speech","Language"],"title":"Transfer learning from speech to music: towards language-sensitive emotion recognition models","type":"publication"},{"authors":["Sascha Grollmisch","Estefanía Cano","Christian Kehling","Michael Taenzer"],"categories":[],"content":"Detailed results can be found in here.\n","date":1578009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578009600,"objectID":"7c44069f095841330617d07cec6f8d6e","permalink":"https://ecanoc.github.io/publication/eusipco20-1/","publishdate":"2019-08-01T19:28:09+08:00","relpermalink":"/publication/eusipco20-1/","section":"publication","summary":"In the context of deep learning, the availability of large amounts of training data can play a critical role in a model's performance. Transfer learning has shown to be a powerful method in which models are first pre-trained for a task where abundant data is available, and then fine-tuned for a separate task where only a limited amount of data exists. In the past years, several models for audio classification have been pre-trained in a supervised or self-supervised fashion to learn complex feature representations, so called embeddings. These embeddings can then be extracted from smaller datasets and used to train subsequent classifiers. In the field of audio event detection (AED) for example, classifiers using these features have achieved high accuracy without the need of additional domain knowledge. This paper evaluates three state-of-the-art embeddings on six audio classification tasks from the fields of music information retrieval and industrial sound analysis, and presents a detailed overview of their potential. The embeddings are systematically evaluated by analyzing the influence of classifier architecture, fusion methods for file-wise predictions, amount of training data, and trained domain on classification accuracy. To better understand the effect of pre-training, results are also compared with those obtained with models trained from scratch. On average, OpenL3 embeddings performed best with a linear SVM classifier and for a reduced number of training examples they outperform the initial baseline.","tags":["Deep learning","Speech-music discrimination","Ensemble size classification","Instrument recognition"],"title":"Analyzing the potential of pre-trained embeddings for audio classification tasks","type":"publication"},{"authors":["Stylianos Mimilakis","Konstantinos Drossos","Estefanía Cano","Gerald Schuller"],"categories":[],"content":"","date":1566309362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566309362,"objectID":"7b99ba058494ce01367e82489efb6d6a","permalink":"https://ecanoc.github.io/publication/talsp20/","publishdate":"2019-08-20T21:56:02+08:00","relpermalink":"/publication/talsp20/","section":"publication","summary":"The goal of this article is to investigate what singing voice separation approaches based on neural networks learn from the data. We examine the mapping functions of neural networks based on the denoising autoencoder (DAE) model that are conditioned on the mixture magnitude spectra. To approximate the mapping functions, we propose an algorithm inspired by the knowledge distillation, denoted the neural couplings algorithm (NCA). The NCA yields a matrix that expresses the mapping of the mixture to the target source magnitude information. Using the NCA, we examine the mapping functions of three fundamental DAE-based models in music source separation; one with single-layer encoder and decoder, one with multi-layer encoder and single-layer decoder, and one using skip-filtering connections (SF) with a single-layer encoding and decoding. We first train these models with realistic data to estimate the singing voice magnitude spectra from the corresponding mixture. We then use the optimized models and test spectral data as input to the NCA. Our experimental findings show that approaches based on the DAE model learn scalar filtering operators, exhibiting a predominant diagonal structure in their corresponding mapping functions, limiting the exploitation of inter-frequency structure of music data. In contrast, skip-filtering connections are shown to assist the DAE model in learning filtering operators that exploit richer inter-frequency structures","tags":[],"title":"Examining the Mapping Functions of Denoising Autoencoders in Singing Voice Separation","type":"publication"},{"authors":["Estefanía Cano","Derry Fitzgerald","Antoine Liutkus","Mark Plumbley","Fabian Stöter"],"categories":[],"content":"","date":1566309362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566309362,"objectID":"25c976123522907572e6eb2dd93867fe","permalink":"https://ecanoc.github.io/publication/spm19/","publishdate":"2019-08-20T21:56:02+08:00","relpermalink":"/publication/spm19/","section":"publication","summary":"Many people listen to recorded music as part of their everyday lives, e.g., from radio or TV programs, compact discs, downloads, or, increasingly, online streaming services. Sometimes we might want to remix the balance within the music, perhaps to make the vocals louder or to suppress an unwanted sound, or we might want to upmix a two-channel stereo recording to a 5.1-channel surround sound system. We might also want to change the spatial location of a musical instrument within the mix. All of these applications are relatively straightforward, provided we have access to separate sound channels (stems) for each musical audio object.","tags":[],"title":"Musical Source Separation: An Introduction","type":"publication"},{"authors":["Estefanía Cano","Scott Beveridge"],"categories":[],"content":"","date":1566309353,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566309353,"objectID":"b0ed31dba073977f2ca680c815cde4b4","permalink":"https://ecanoc.github.io/publication/ismir19/","publishdate":"2019-08-20T21:55:53+08:00","relpermalink":"/publication/ismir19/","section":"publication","summary":"This work aims to characterize microtiming variations in traditional Shetland fiddle music. These microtiming variations dictate the rhythmic flow of a performed melody, and contribute, among other things, to the suitability of this music as an accompaniment to dancing. In the context of Shetland fiddle music, these microtiming variations are often referred to as lilt. Using a corpus of 27 traditional fiddle tunes from the Shetland Isles, we examine inter-beat timing deviations, as well as inter-onset timing deviations of eighth note sequences. Results show a number of distinct inter-beat and inter-onset rhythmic patterns that may characterize lilt, as well as idiosyncratic patterns for each performer. This paper presents a first step towards the use of Music Information Retrieval (MIR) techniques for modelling lilt in traditional Scottish fiddle music, and highlights its implications in the field of ethnomusicology.","tags":[],"title":"Microtiming analyisis in traditional Shetland Fiddle Music","type":"publication"},{"authors":["Estefanía Cano","Hanna Lukashevich"],"categories":[],"content":"","date":1566309335,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566309335,"objectID":"04d93c4d9f5937a25c2d19e462e8e247","permalink":"https://ecanoc.github.io/publication/mmsp19/","publishdate":"2019-08-20T21:55:35+08:00","relpermalink":"/publication/mmsp19/","section":"publication","summary":"Selective hearing (SH) refers to the listeners' capability to focus their attention on a specific sound source or a group of sound sources in their auditory scene. This in turn implies that the listeners' focus is minimized for sources that are of no interest. This paper describes the current landscape of machine listening research, and outlines ways in which these technologies can be leveraged to achieve SH with computational means. To do so, a brief overview of the state-of-the-art in the fields of  detection, classification, separation, localization and enhancement of sound sources is presented, highlighting recent advances in each field, and drawing connections between them. Two main challenges lie ahead in the development of SH applications: (1) Unified methods that can jointly detect/classify/localize and separate/enhance  sound sources are required to provide both the flexibility and robustness required for real-life SH. (2) Low-latency methods suitable for real-time performance are critical when dealing with the dynamic nature of real-life auditory scenes. ","tags":[],"title":"Selective Hearing: A Machine Listening perspective","type":"publication"},{"authors":["Sascha Grollmisch","Estefanía Cano","Fernando Mora-Ángel","Gustavo López Gil"],"categories":[],"content":"","date":1564659243,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564659243,"objectID":"250ce45e6819242f89e72b27964d85d6","permalink":"https://ecanoc.github.io/publication/cmmr19/","publishdate":"2019-08-01T19:34:03+08:00","relpermalink":"/publication/cmmr19/","section":"publication","summary":"Reliable methods for automatic retrieval of semantic information from large digital music archives can play a critical role in musicological research and musical heritage preservation. With the advancement of machine learning techniques, new possibilities for information retrieval in scenarios where ground-truth data is scarce are now available. This work investigates the problem of counting the number of instruments in music recordings as a classification task. For this purpose, a new data set of Colombian Andean string music was compiled and annotated by  expert musicologists. Different neural network architectures, as well as pre-processing steps and data augmentation techniques were systematically evaluated and optimized. The best deep neural network architecture achieved 80.7% file-wise accuracy using only feed forward layers with linear magnitude spectrograms as input representation. This model will serve as a baseline for future research on ensemble size classification.","tags":[],"title":"Ensemble size classification in Colombian Andean string music recordings","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6e9337a8bb71063b8b153d50dbcb18cf","permalink":"https://ecanoc.github.io/mir/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/mir/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5bffc11e93abf962c44d02beef459d17","permalink":"https://ecanoc.github.io/viewer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/viewer/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5681b77a6a24f93338b24898bd7dae8c","permalink":"https://ecanoc.github.io/project/acmus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/acmus/","section":"project","summary":"Computational musicology, machine learning and Colombian music.","tags":["Computational Musicology","Deep Learning","MIR","Topics"],"title":"ACMus","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eedad02d16858bdae4459541cdd24a04","permalink":"https://ecanoc.github.io/project/musiced/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/musiced/","section":"project","summary":"Technologies and MIR for music education.","tags":["Music Education","Topics"],"title":"Music Education","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"46fb1a84e2a25d81b37cf6f8ca3acb1d","permalink":"https://ecanoc.github.io/project/mir/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/mir/","section":"project","summary":"MIR research, interests and projects","tags":["Topics"],"title":"Music Information Retrieval","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e29eb2329ab1c5fcbe3b876d3493deea","permalink":"https://ecanoc.github.io/project/soundseparation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/soundseparation/","section":"project","summary":"Sound source separation applied to music and audio.","tags":["Source Separation","Topics"],"title":"Sound Source Separation","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1980c3cf44df04b8ee20d1de542c26b9","permalink":"https://ecanoc.github.io/project/fiddle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/fiddle/","section":"project","summary":"Analysis of micro-timing variations in Shetland fiddle music.","tags":["Computational Musicology","MIR"],"title":"The Scottish fiddle","type":"project"}]